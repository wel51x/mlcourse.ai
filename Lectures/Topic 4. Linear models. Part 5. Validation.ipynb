{"cells":[{"metadata":{"_uuid":"c95c64ea101509a5efae5b76488990e9dce94d74"},"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/webt/ia/m9/zk/iam9zkyzqebnf_okxipihkgjwnw.jpeg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course\n\nAuthor: [Yury Kashnitsky](https://www.linkedin.com/in/festline). Translated and edited by [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Nerses Bagiyan](https://www.linkedin.com/in/nersesbagiyan/), [Yulia Klimushina](https://www.linkedin.com/in/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-commercial purpose.\n\n*You can also check out the latest version of this notebook in the [course repository](https://github.com/Yorko/mlcourse.ai), and the corresponding video lectures: [theoretical part](https://www.youtube.com/watch?v=l3jiw-N544s), [practical part](https://www.youtube.com/watch?v=7o0SWgY89i8).*"},{"metadata":{"_uuid":"f3ff070ec1754f67bfd59b37977bfe5e651b1feb"},"cell_type":"markdown","source":"# <center>Topic 4. Linear Classification and Regression\n## <center> Part 5. Validation and Learning Curves"},{"metadata":{"_uuid":"cf7c13c494ef2f0d23437f987c1a2b9b294bc0d3","trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\nfrom sklearn.model_selection import validation_curve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42ea067eab6df9622ef420e78739bca10f49189c"},"cell_type":"markdown","source":"Now that we have an idea of model validation, cross-validation, and regularization. Let's consider the bigger question:\n\n**What to do if the quality of the model is dissatisfying?**\n\n- Should we make the model more complicated or more simple?\n- Should we add more features?\n- Do we simply need more data for training?\n\nThe answers to these questions are not obvious. In particular, sometimes a more complex model can lead to a deterioration in performance. Other times, adding new observations will not bring noticeable changes. In fact, the ability to make the right decision and choose the right method to improve the model distinguishes a good professional from a bad one."},{"metadata":{"_uuid":"d1ccebefdaa577c79c09d5cfaa00608ae2367add"},"cell_type":"markdown","source":"We will work our data on customer churn of telecom operator."},{"metadata":{"_uuid":"b74fe648505e38d7071693573d73f14018cadcb2","trusted":false},"cell_type":"code","source":"data = pd.read_csv('../input/telecom_churn.csv').drop('State', axis=1)\ndata['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\n\ny = data['Churn'].astype('int').values\nX = data.drop('Churn', axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"321a00a3a8ca19cafb364017f0f09e2a743e00ae"},"cell_type":"markdown","source":"**We will train logistic regression with stochastic gradient descent. Later in the course, we will have a separate article on this topic.**"},{"metadata":{"_uuid":"aa24d34f9d7ab5cf8c5ae34bfed8c6543692b13d","trusted":false},"cell_type":"code","source":"alphas = np.logspace(-2, 0, 20)\nsgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17)\nlogit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=2)), \n                       ('sgd_logit', sgd_logit)])\nval_train, val_test = validation_curve(logit_pipe, X, y,\n                                       'sgd_logit__alpha', alphas, cv=5,\n                                        scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f27e180ec630bb9758a94ce81ef2d3baf7de97a"},"cell_type":"markdown","source":"**As a first step, we will construct validation curves showing how the quality (ROC-AUC) on training and test sets varies with the regularization parameter.**"},{"metadata":{"_uuid":"447082218a609a86e25532085809b391c08d19d1","trusted":false},"cell_type":"code","source":"def plot_with_err(x, data, **kwargs):\n    mu, std = data.mean(1), data.std(1)\n    lines = plt.plot(x, mu, '-', **kwargs)\n    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n                     facecolor=lines[0].get_color(), alpha=0.2)\n\nplot_with_err(alphas, val_train, label='training scores')\nplot_with_err(alphas, val_test, label='validation scores')\nplt.xlabel(r'$\\alpha$'); plt.ylabel('ROC AUC')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95f3cb1f99eb5014617fa46431025889a1f451fb"},"cell_type":"markdown","source":"The trend is quite visible and is very common.\n\n- For simple models, training and validation errors are close and large. This suggests that the model **underfitted**, meaning it does not have a sufficient number of parameters.\n\n- For highly sophisticated models, training and validation errors differ significantly. This can be explained by **overfitting**. When there are too many parameters or regularization is not strict enough, the algorithm can be \"distracted\" by the noise in the data and lose track of the overall trend.\n\n"},{"metadata":{"_uuid":"be1da9d827a0be77301c731a7d37c54bf8380990"},"cell_type":"markdown","source":"### How much data is needed?\n\nThe more data the model uses, the better. But how do we understand whether new data will helpful in any given situation? For example, is it rational to spend $N$ for assessors to double the dataset?\n\nSince the new data can be unavailable, it is reasonable to vary the size of the training set and see how the quality of the solution depends on the amount of training data. This is how we get **learning curves**.\n\nThe idea is simple: we display the error as a function of the number of examples used in training. The parameters of the model are fixed in advance."},{"metadata":{"_uuid":"403780bf914accb1b65617674a3bc661980e83e3","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(degree=2, alpha=0.01):\n    train_sizes = np.linspace(0.05, 1, 20)\n    logit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=degree)), \n                           ('sgd_logit', SGDClassifier(n_jobs=-1, random_state=17, alpha=alpha))])\n    N_train, val_train, val_test = learning_curve(logit_pipe,\n                                                  X, y, train_sizes=train_sizes, cv=5,\n                                                  scoring='roc_auc')\n    plot_with_err(N_train, val_train, label='training scores')\n    plot_with_err(N_train, val_test, label='validation scores')\n    plt.xlabel('Training Set Size'); plt.ylabel('AUC')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e5d546b6f7fc1fa4fe3dcaa6759a350c66202df"},"cell_type":"markdown","source":"Let's see what we get for the linear model. We will set the regularization coefficient to be quite large."},{"metadata":{"_uuid":"28d106b5b0995edc637d1512f25d9d7845c63fe0","trusted":false},"cell_type":"code","source":"plot_learning_curve(degree=2, alpha=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08ffb19c9f8c090fcd7f59030175a1b25adea626"},"cell_type":"markdown","source":"A typical situation: for a small amounts of data, errors between training and cross-validation sets are quite different, indicating overfitting. For that same model but with a large amount of data, errors \"converge\", indicating underfitting.\n \nIf we add more data, error on the training set will not grow. On the other hand, the error on the test data will not be reduced.\n \nSo, we see that the errors \"converged\", and the addition of new data will not help. Actually this case is the most interesting for business. It is possible that we increase the size of the dataset by 10x, but, without changing the complexity of the model, this additional data may not help. Therefore the strategy of \"set once, then use 10 times\" might not work.\n \nWhat happens if we reduce the regularization coefficient to 0.05?\n \nWe see a good trend - the curves gradually converge, and if we move farther to the right i.e. add more data to the model, we can improve the quality on the validation set even more. "},{"metadata":{"_uuid":"cd9ea91a7b81533d47c3c2d644254f6d517050c3","trusted":false},"cell_type":"code","source":"plot_learning_curve(degree=2, alpha=0.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feced9c06804938f9cf0b0a6a139ab4130a1f75d"},"cell_type":"markdown","source":"Now, what if we make the model even more complex by setting alpha = 10-4?\n\nOverfitting is seen - AUC decreases on both the training and the validation sets."},{"metadata":{"_uuid":"27b5e9c28a7dad8e52824cfcef646257e541d48f","trusted":false},"cell_type":"code","source":"plot_learning_curve(degree=2, alpha=1e-4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f87019b2448b21809df1b0962cdcb044d91467f"},"cell_type":"markdown","source":"Constructing these curves can help understand which way to go and how to properly adjust the complexity of the model for new data."},{"metadata":{"_uuid":"49af4be78ba6e0021931f0123f729e0296f733d4"},"cell_type":"markdown","source":"**Conclusions on the learning and validation curves:**\n\n\n- Error on the training set says nothing about the quality of the model by itself\n- Cross-validation error shows how well the model fits the data (the existing trend in the data) while retaining the ability to generalize to new data\n- **Validation curve** is a graph showing the results on training and validation sets depending on the **complexity of the model**:\n    + if the two curves are close to each other and both errors are large, it is a sign of *underfitting*\n    + if the two curves are far from each other, it is a sign of *overfitting*\n- **Learning Curve** is a graph showing the results on training and validation sets depending on the number of observations:\n    + if the curves converge, adding new data won't help, and it is necessary to change the complexity of the model \n    + if the curves have not converged, adding new data can improve the result\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Demo assignment\nTo practice with linear models, you can complete [this assignment](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit) where you'll build a sarcasm detection model. The assignment is just for you to practice, and goes with [solution](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution)."},{"metadata":{"_uuid":"74306d596d7d47975ab35794c24e2a6a4fe2ba3c"},"cell_type":"markdown","source":"## Useful resources\n- Medium [\"story\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) based on this notebook\n- Main course [site](https://mlcourse.ai), [course repo](https://github.com/Yorko/mlcourse.ai), and YouTube [channel](https://www.youtube.com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n- Course materials as a [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse)\n- If you read Russian: an [article](https://habrahabr.ru/company/ods/blog/323890/) on Habr.com with ~ the same material. And a [lecture](https://youtu.be/oTXGQ-_oqvI) on YouTube\n- A nice and concise overview of linear models is given in the book [\"Deep Learning\"](http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio, and A. Courville).\n- Linear models are covered practically in every ML book. We recommend \"Pattern Recognition and Machine Learning\" (C. Bishop) and \"Machine Learning: A Probabilistic Perspective\" (K. Murphy).\n- If you prefer a thorough overview of linear model from a statistician's viewpoint, then look at \"The elements of statistical learning\" (T. Hastie, R. Tibshirani, and J. Friedman).\n- The book \"Machine Learning in Action\" (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) library. These guys work hard on writing really clear documentation.\n- Scipy 2017 [scikit-learn tutorial](https://github.com/amueller/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n- One more [ML course](https://github.com/diefimov/MTH594_MachineLearning) with very good materials.\n- [Implementations](https://github.com/rushter/MLAlgorithms) of many ML algorithms. Search for linear regression and logistic regression.\n\n## Support course creators\n<br>\n<center>\nYou can make a monthly (Patreon) or one-time (Ko-Fi) donation â†“\n\n<br>\n<br>\n\n<a href=\"https://www.patreon.com/ods_mlcourse\">\n<img src=\"https://habrastorage.org/webt/zc/11/0y/zc110yh0u3kgnlmay1gwbekk0ys.png\" width=20% />\n\n<br>\n\n<a href=\"https://ko-fi.com/mlcourse_ai\">\n<img src=\"https://habrastorage.org/webt/8r/ml/xf/8rmlxfpdzukegpxa62cxlfvgkqe.png\" width=20% />\n    \n</center>"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"name":"lesson7_part5_overfitting_validation.ipynb"},"nbformat":4,"nbformat_minor":1}