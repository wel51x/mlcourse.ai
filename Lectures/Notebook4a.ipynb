{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n",
      "data/credit_scoring_train.csv\n",
      "data/video_games_sales.csv\n",
      "data/responses.csv\n",
      "data/agaricus-lepiota.data.txt\n",
      "data/top10_tags.tsv\n",
      "data/bank.csv\n",
      "data/adult.data.csv\n",
      "data/flight_delays_test.csv\n",
      "data/billdoard_dataset.csv\n",
      "data/weights_heights.csv\n",
      "data/tutorial_data.pickle\n",
      "data/ig_users.csv\n",
      "data/hmeq.csv\n",
      "data/drugs-and-math.csv\n",
      "data/hostel_factors.csv\n",
      "data/nba_2013.csv\n",
      "data/ads_hour.csv\n",
      "data/bank_train_target.csv\n",
      "data/prolongation_service_train.csv\n",
      "data/adult_train.csv\n",
      "data/mlbootcamp5_train.csv\n",
      "data/spooky_writer_train.csv\n",
      "data/currency.csv\n",
      "data/credit_scoring_test.csv\n",
      "data/vgsales.csv\n",
      "data/chronic_kidney_disease.csv\n",
      "data/accidental-deaths-in-usa-monthly.csv\n",
      "data/pubs.json\n",
      "data/LasVegasTripAdvisorReviews-Dataset.csv\n",
      "data/girls.csv\n",
      "data/titanic_test.csv\n",
      "data/dengue_labels_train.csv\n",
      "data/candy_production.csv\n",
      "data/Rail_Insurance_Claims.csv\n",
      "data/spooky_writer_test.csv\n",
      "data/spooky_authors.csv\n",
      "data/hour_online.csv\n",
      "data/dengue_features_train.csv\n",
      "data/flight_delays_train.csv\n",
      "data/scores.csv\n",
      "data/breast_cancer.csv\n",
      "data/1year.arff\n",
      "data/mlcourse_open_first_survey_data.csv\n",
      "data/telecom_churn.csv\n",
      "data/bikes_rent.csv\n",
      "data/habr_sample_submission.csv\n",
      "data/microchip_tests.txt\n",
      "data/spooky_writer_sample_submission.csv\n",
      "data/FB_data.csv\n",
      "data/ehresp_2014.csv\n",
      "data/site_dic.pkl\n",
      "data/prolongation_service_test.csv\n",
      "data/dengue_features_test.csv\n",
      "data/beauty.csv\n",
      "data/credit_scoring_sample.csv\n",
      "data/student-mat.csv\n",
      "data/clikstream_data.csv\n",
      "data/ads.csv\n",
      "data/medium_posts.csv.zip\n",
      "data/winequality-white.csv\n",
      "data/horse.csv\n",
      "data/cars.csv\n",
      "data/mlcourse_open_first_survey_parsed.csv\n",
      "data/DSL-StrongPasswordData.csv.zip\n",
      "data/wiki_machine_learning.csv\n",
      "data/adult_test.csv\n",
      "data/titanic_train.csv\n",
      "data/spam.csv\n",
      "data/bank_train.csv\n",
      "data/DJIA_table.csv\n",
      "data/kaggle_catch-me-if-you-can/test_sessions.csv\n",
      "data/kaggle_catch-me-if-you-can/train_sessions.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "%cd /notebooks\n",
    "datadir = \"data/\"\n",
    "kaggledir = datadir + \"kaggle_catch-me-if-you-can\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(datadir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# a helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/catch-me-if-you-can/train_sessions.csv',\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv('../input/catch-me-if-you-can/test_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "\n",
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites].fillna(0).astype('int').to_csv('train_sessions_text.txt', \n",
    "                                               sep=' ', \n",
    "                       index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').to_csv('test_sessions_text.txt', \n",
    "                                              sep=' ', \n",
    "                       index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 train_sessions_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('train_sessions_text.txt') as inp_train_file:\n",
    "    X_train = cv.fit_transform(inp_train_file)\n",
    "with open('test_sessions_text.txt') as inp_test_file:\n",
    "    X_test = cv.transform(inp_test_file)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv= 5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_logit1 = logit.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(test_pred_logit1, 'logit_sub1.txt') ## .908 ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head logit_sub1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Features\n",
    "\n",
    "- hour when the session started\n",
    "- morning \n",
    "- day\n",
    "- eve\n",
    "- night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(time1_series, X_sparse):\n",
    "    hour = time1_series.apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    X = hstack([X_sparse, morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1), evening.values.reshape(-1, 1), \n",
    "                night.values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'time1'].fillna(0).apply(lambda ts: ts.hour).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_with_time = add_time_features(train_df['time1'].fillna(0), X_train)\n",
    "X_test_with_time = add_time_features(test_df['time1'].fillna(0), X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_with_time = LogisticRegression(C = 1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_scores = cross_val_score(logit_with_time, X_train_with_time, y_train, cv= 5, scoring='roc_auc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_with_time.fit(X_train_with_time, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_logit2 = logit_with_time.predict_proba(X_test_with_time)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(test_pred_logit2, 'logit_sub2.txt') ## .93565 ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head logit_sub2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
