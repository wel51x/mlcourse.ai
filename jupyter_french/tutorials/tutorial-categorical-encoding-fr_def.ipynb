{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<center>\nAuteur: [Wayde Herman](https://www.linkedin.com/in/wayde-herman-10986685/) depuis [kaggle](https://www.kaggle.com/waydeherman/tutorial-categorical-encoding). Traduit et édité par [Ousmane Cissé](https://www.linkedin.com/in/ousmane-cissé).  \n<center>\nCe matériel est soumis aux termes et conditions de la licence [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).  \nL'utilisation gratuite est autorisée à des fins non commerciales."
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "# Encodage des caractéristiques catégorielles\n\n## Introduction:\n\nDans la plupart des problèmes de science des données, nos ensembles de données contiendront des caractéristiques catégorielles. Les entités catégorielles contiennent un nombre fini de valeurs discrètes. La façon dont nous représentons ces caratéristiques aura un impact sur les performances de notre modèle. Comme dans d'autres aspects de l'apprentissage automatique, il n'y a pas de \"baguette magique\". Déterminer la bonne approche, spécifique à notre modèle et à nos données, fait partie du défi.\n\nCe tutoriel vise à couvrir quelques-unes de ces méthodes. Nous commençons par couvrir une technique simple avant d'aborder des approches plus complexes et moins connues.\n\n**Liste des méthodes couvertes**:\n1. One-Hot Encoding (Encodage 1 parmi n)\n2. Feature Hashing (Hachage de caractéristiques)\n3. Binary Encoding (Encodage binaire)\n4. Target Encoding (Encodage de la cible)\n5. Weight of Evidence (Poids de l'élement preuve)"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-02-16T10:46:31.179058Z",
          "end_time": "2020-02-16T10:46:52.352445Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import required libraries:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Set our random seed:\nSEED = 17\nPATH_TO_DIR = 'data/'\n\nprint(os.listdir(PATH_TO_DIR))",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['grades', 'test.csv', 'pima-diabetes', 'train.csv', 'sampleSubmission.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Pour ce tutoriel, nous utiliserons l'ensemble de données «[Amazon.com Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)». Cet ensemble de données de classification binaire est composé de caractéristiques strictement catégorielles, qui sont déjà converties en chiffres, ce qui en fait un choix particulièrement approprié pour explorer diverses techniques de codage. Pour simplifier les choses, nous n'utiliserons qu'un sous-ensemble des caractéristiques de cette démonstration."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:46:57.345993Z",
          "end_time": "2020-02-16T10:46:58.159018Z"
        }
      },
      "cell_type": "code",
      "source": "# Import data:\ntrain = pd.read_csv(PATH_TO_DIR + 'train.csv')",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:46:58.909242Z",
          "end_time": "2020-02-16T10:46:58.941102Z"
        }
      },
      "cell_type": "code",
      "source": "y = train['ACTION']\ntrain = train[['RESOURCE', 'MGR_ID', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE']]",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Nous comparerons les différences de ces méthodes de codage à la fois sur un modèle linéaire et sur un modèle basé sur des arbres. Ceux-ci représentent deux familles de modèles qui ont des comportements contrastés en ce qui concerne les différentes représentations d'entités."
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:00.906551Z",
          "end_time": "2020-02-16T10:47:00.924109Z"
        }
      },
      "cell_type": "code",
      "source": "logit = LogisticRegression(random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:01.735393Z",
          "end_time": "2020-02-16T10:47:01.773684Z"
        }
      },
      "cell_type": "code",
      "source": "# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.2, random_state=SEED)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:03.661091Z",
          "end_time": "2020-02-16T10:47:03.690033Z"
        }
      },
      "cell_type": "code",
      "source": "# We create a helper function to get the scores for each encoding method:\ndef get_score(model, X, y, X_val, y_val):\n    model.fit(X, y)\n    y_pred = model.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    return score",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:04.625134Z",
          "end_time": "2020-02-16T10:47:04.681445Z"
        }
      },
      "cell_type": "code",
      "source": "# Lets have a quick look at our data:\nX_train.head(5)",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "       RESOURCE  MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n22434     29071    5178            158101       118424     118828\n24163     20222    5550            119235       292795     118997\n10066     79092    6047            279443       308574     118779\n19869     14570   51104            189996        19721     118570\n1855      32642   18097            130662       292795     117948",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RESOURCE</th>\n      <th>MGR_ID</th>\n      <th>ROLE_FAMILY_DESC</th>\n      <th>ROLE_FAMILY</th>\n      <th>ROLE_CODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22434</th>\n      <td>29071</td>\n      <td>5178</td>\n      <td>158101</td>\n      <td>118424</td>\n      <td>118828</td>\n    </tr>\n    <tr>\n      <th>24163</th>\n      <td>20222</td>\n      <td>5550</td>\n      <td>119235</td>\n      <td>292795</td>\n      <td>118997</td>\n    </tr>\n    <tr>\n      <th>10066</th>\n      <td>79092</td>\n      <td>6047</td>\n      <td>279443</td>\n      <td>308574</td>\n      <td>118779</td>\n    </tr>\n    <tr>\n      <th>19869</th>\n      <td>14570</td>\n      <td>51104</td>\n      <td>189996</td>\n      <td>19721</td>\n      <td>118570</td>\n    </tr>\n    <tr>\n      <th>1855</th>\n      <td>32642</td>\n      <td>18097</td>\n      <td>130662</td>\n      <td>292795</td>\n      <td>117948</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:09.893424Z",
          "end_time": "2020-02-16T10:47:09.936333Z"
        }
      },
      "cell_type": "code",
      "source": "X_train.info()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 26215 entries, 22434 to 10863\nData columns (total 5 columns):\nRESOURCE            26215 non-null int64\nMGR_ID              26215 non-null int64\nROLE_FAMILY_DESC    26215 non-null int64\nROLE_FAMILY         26215 non-null int64\nROLE_CODE           26215 non-null int64\ndtypes: int64(5)\nmemory usage: 1.2 MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:11.120458Z",
          "end_time": "2020-02-16T10:47:11.151378Z"
        }
      },
      "cell_type": "code",
      "source": "# Discover the number of categories within each categorical feature:\nlen(X_train.RESOURCE.unique()), len(X_train.MGR_ID.unique()), len(X_train.ROLE_FAMILY_DESC.unique()), len(X_train.ROLE_FAMILY.unique()),len(X_train.ROLE_CODE.unique())",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "(6711, 4062, 2201, 67, 337)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:12.277781Z",
          "end_time": "2020-02-16T10:47:12.30169Z"
        }
      },
      "cell_type": "code",
      "source": "# Create a list of each categorical column name:\ncolumns = [i for i in X_train.columns]",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-02-16T10:47:13.102726Z",
          "end_time": "2020-02-16T10:47:13.121818Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "columns",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "['RESOURCE', 'MGR_ID', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Avant de commencer, examinons la vitesse et les performances de l'entraînement de ces modèles sans encodage de caractéristiques."
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T09:22:58.627509Z",
          "end_time": "2020-02-16T09:22:58.723386Z"
        }
      },
      "cell_type": "code",
      "source": "%%time\nbaseline_logit_score = get_score(logit, X_train, y_train, X_val, y_val)\nprint('Logistic Regression score without feature engineering:', baseline_logit_score)",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Logistic Regression score without feature engineering: 0.5350998573215817\nCPU times: user 97.6 ms, sys: 0 ns, total: 97.6 ms\nWall time: 90.3 ms\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T09:23:00.87044Z",
          "end_time": "2020-02-16T09:23:01.763607Z"
        }
      },
      "cell_type": "code",
      "source": "%%time\nbaseline_rf_score = get_score(rf, X_train, y_train, X_val, y_val)\nprint('Random Forest score without feature engineering:', baseline_rf_score)",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Random Forest score without feature engineering: 0.7785989401031782\nCPU times: user 889 ms, sys: 0 ns, total: 889 ms\nWall time: 888 ms\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "## One-Hot Encoding:"
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "La première méthode que nous aborderons est celle que vous connaissez sans doute. Le One-hot encoding transforme 1 caratéristique catégorielle composée de m catégories en m* caractéristiques distinctes avec des valeurs de 0 ou 1.\n\nIl existe 2 façons de mettre en œuvre unOne-Hot Encoding, avec pandas ou scikit-learn. Dans ce tutoriel, nous avons choisi d'utiliser ce dernier.  \n\n*En fait, il est considéré comme plus correct d'élargir m catégories en (m - 1) caractéristiques distinctes. La raison en est double. Premièrement, si les valeurs des (m - 1) caractéristiques sont connues, la m-ième caractéristiques peut être déduite et deuxièmement parce que l'inclusion de la m-ième entité peut rendre certains modèles linéaires instables. Plus d'informations à ce sujet peuvent être trouvées [ici](https://www.algosome.com/articles/dummy-variable-trap-regression.html). En pratique, je pense que cela dépend de votre modèle. Certains modèles non linéaires fonctionnent mieux avec les caractéristiques m."
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T09:23:09.218092Z",
          "end_time": "2020-02-16T09:23:09.234993Z"
        }
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\n\none_hot_enc = OneHotEncoder(sparse=False)",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T09:23:10.393678Z",
          "end_time": "2020-02-16T09:24:57.049525Z"
        }
      },
      "cell_type": "code",
      "source": "print('Original number of features: \\n', X_train.shape[1], \"\\n\")\ndata_ohe_train = (one_hot_enc.fit_transform(X_train))\ndata_ohe_val = (one_hot_enc.transform(X_val))\nprint('Features after OHE: \\n', data_ohe_train.shape[1])",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Original number of features: \n 5 \n\nFeatures after OHE: \n 13378\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T09:25:31.747943Z",
          "end_time": "2020-02-16T09:26:02.918421Z"
        }
      },
      "cell_type": "code",
      "source": "%%time\nohe_logit_score = get_score(logit, data_ohe_train, y_train, data_ohe_val, y_val)\nprint('Logistic Regression score with one-hot encoding:', ohe_logit_score)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Logistic Regression score with one-hot encoding: 0.8658368212232391\nCPU times: user 31.8 s, sys: 0 ns, total: 31.8 s\nWall time: 31.1 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nohe_rf_score = get_score(rf, data_ohe_train, y_train, data_ohe_val, y_val)\nprint('Random Forest score with one-hot encoding:', ohe_rf_score)",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with one-hot encoding: 0.8151528170201999\nCPU times: user 3min 58s, sys: 4.14 s, total: 4min 2s\nWall time: 4min 2s\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Comme nous pouvons le voir, bien que les performances du modèle se soient améliorées, l'entraînement a également pris plus de temps. Cela est dû à l'augmentation du nombre de caractéristiques. Les coûts de calcul ne sont pas le seul problème associé à l'augmentation des dimensions. Un ensemble de données avec plus de caractéristiques nécessitera un modèle avec plus de paramètres qui à son tour nécessitera plus de données pour entraîner ces paramètres. Dans de nombreux cas, tels que les compétitions de kaggle, la taille de nos données est fixe et, par conséquent, la dimensionnalité de nos données devrait toujours être une préoccupation.\n\nUne façon de gérer la dimensionnalité élevée consiste à compresser les caractéristiques. Le hachage de caractéristiques, que nous aborderons ensuite, en est un exemple."
    },
    {
      "metadata": {
        "lang": "en"
      },
      "cell_type": "markdown",
      "source": "## Feature Hashing:"
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Le hachage de caractéristiques mappe chaque catégorie d'une caractéristique catégorielle à un entier dans une plage prédéterminée. Cette plage de sortie est plus petite que la plage d'entrée, de sorte que plusieurs catégories peuvent être mappées sur le même entier. Le hachage des caractéristiques est très similaire au One-Hot Encoding, mais avec un contrôle sur les dimensions de sortie.\n\nPour implémenter le hachage des caractéristiques en python, nous pouvons utiliser category_encoder, une bibliothèque contenant des encodeurs de catégorie compabitables sklearn."
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:48:44.519991Z",
          "end_time": "2020-02-16T10:48:44.541827Z"
        }
      },
      "cell_type": "code",
      "source": "# Install category_encoders:\n# !pip install category_encoders\n# or !conda install -c conda-forge category_encoders -y",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T10:45:49.654694Z",
          "end_time": "2020-02-16T10:45:52.698034Z"
        }
      },
      "cell_type": "code",
      "source": "from category_encoders import HashingEncoder",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "La taille des dimensions de sortie est contrôlée par la variable n_components. Cela peut être traité comme un hyperparamètre."
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T11:03:54.715779Z",
          "end_time": "2020-02-16T11:03:54.735157Z"
        }
      },
      "cell_type": "code",
      "source": "n_components_list = [100, 500, 1000, 5000, 10000]\nn_components_list_str = [str(i) for i in n_components_list]",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fh_logit_scores = []\n\n# Iterate over different n_components:\nfor n_components in n_components_list:\n    \n    hashing_enc = HashingEncoder(cols=columns, n_components=n_components).fit(X_train, y_train)\n    \n    X_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\n    X_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))\n    \n    fe_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\n    fh_logit_scores.append(fe_logit_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T11:31:55.741Z",
          "end_time": "2020-02-16T11:32:55.375586Z"
        }
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(8, 5))\nplt.plot(n_components_list_str, fh_logit_scores, linewidth=3)\nplt.title('n_compontents vs roc_auc for feature hashing with logistic regression')\nplt.xlabel('n_components')\nplt.ylabel('score')\nplt.show;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Comme nous pouvons le voir, les performances du modèle de régression logistique s'améliorent à mesure que le nombre de composants augmente. Mais regardons l'effet de la réduction des dimensions sur un modèle de forêt aléatoire."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "hashing_enc = HashingEncoder(cols=columns, n_components=10000).fit(X_train, y_train)\n\nX_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\nX_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_hashing.head()",
      "execution_count": 23,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col_0</th>\n      <th>col_1</th>\n      <th>col_2</th>\n      <th>col_3</th>\n      <th>col_4</th>\n      <th>col_5</th>\n      <th>col_6</th>\n      <th>col_7</th>\n      <th>col_8</th>\n      <th>col_9</th>\n      <th>col_10</th>\n      <th>col_11</th>\n      <th>col_12</th>\n      <th>col_13</th>\n      <th>col_14</th>\n      <th>col_15</th>\n      <th>col_16</th>\n      <th>col_17</th>\n      <th>col_18</th>\n      <th>col_19</th>\n      <th>col_20</th>\n      <th>col_21</th>\n      <th>col_22</th>\n      <th>col_23</th>\n      <th>col_24</th>\n      <th>col_25</th>\n      <th>col_26</th>\n      <th>col_27</th>\n      <th>col_28</th>\n      <th>col_29</th>\n      <th>col_30</th>\n      <th>col_31</th>\n      <th>col_32</th>\n      <th>col_33</th>\n      <th>col_34</th>\n      <th>col_35</th>\n      <th>col_36</th>\n      <th>col_37</th>\n      <th>col_38</th>\n      <th>col_39</th>\n      <th>...</th>\n      <th>col_9960</th>\n      <th>col_9961</th>\n      <th>col_9962</th>\n      <th>col_9963</th>\n      <th>col_9964</th>\n      <th>col_9965</th>\n      <th>col_9966</th>\n      <th>col_9967</th>\n      <th>col_9968</th>\n      <th>col_9969</th>\n      <th>col_9970</th>\n      <th>col_9971</th>\n      <th>col_9972</th>\n      <th>col_9973</th>\n      <th>col_9974</th>\n      <th>col_9975</th>\n      <th>col_9976</th>\n      <th>col_9977</th>\n      <th>col_9978</th>\n      <th>col_9979</th>\n      <th>col_9980</th>\n      <th>col_9981</th>\n      <th>col_9982</th>\n      <th>col_9983</th>\n      <th>col_9984</th>\n      <th>col_9985</th>\n      <th>col_9986</th>\n      <th>col_9987</th>\n      <th>col_9988</th>\n      <th>col_9989</th>\n      <th>col_9990</th>\n      <th>col_9991</th>\n      <th>col_9992</th>\n      <th>col_9993</th>\n      <th>col_9994</th>\n      <th>col_9995</th>\n      <th>col_9996</th>\n      <th>col_9997</th>\n      <th>col_9998</th>\n      <th>col_9999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   col_0  col_1  col_2  col_3    ...     col_9996  col_9997  col_9998  col_9999\n0      0      0      0      0    ...            0         0         0         0\n1      0      0      0      0    ...            0         0         0         0\n2      0      0      0      0    ...            0         0         0         0\n3      0      0      0      0    ...            0         0         0         0\n4      0      0      0      0    ...            0         0         0         0\n\n[5 rows x 10000 columns]"
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nhashing_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\nprint('Logistic Regression score with feature hashing:', hashing_logit_score)",
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with feature hashing: 0.8526684993463501\nCPU times: user 8.37 s, sys: 12 ms, total: 8.38 s\nWall time: 8.31 s\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nhashing_rf_score = get_score(rf, X_train_hashing, y_train, X_val_hashing, y_val)\nprint('Random Forest score with feature hashing:', hashing_rf_score)",
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with feature hashing: 0.82298563375926\nCPU times: user 27.3 s, sys: 268 ms, total: 27.5 s\nWall time: 27.2 s\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Cela s'améliore! Comme nous l'avons peut-être deviné, la réduction du nombre de caractéristiques améliore les performances des modèles basés sur les arbres."
    },
    {
      "metadata": {
        "lang": "en"
      },
      "cell_type": "markdown",
      "source": "## Binary Encoding:"
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "L' encodage binaire implique la conversion de chaque catégorie en un code binaire, par exemple 2 devient 11 et 3 devient 100, puis divise la chaîne binaire résultante en colonnes.\n\nCela peut être plus facile à comprendre avec un exemple:"
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-02-16T11:00:54.294954Z",
          "end_time": "2020-02-16T11:00:54.439681Z"
        },
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create example dataframe with numbers ranging from 1 to 5:\nexample_df = pd.DataFrame([1,2,3,4,5], columns=['example'])\n\nfrom category_encoders import BinaryEncoder\n\nexample_binary = BinaryEncoder(cols=['example']).fit_transform(example_df)\n\nexample_binary",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "   example_0  example_1  example_2  example_3\n0          0          0          0          1\n1          0          0          1          0\n2          0          0          1          1\n3          0          1          0          0\n4          0          1          0          1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>example_0</th>\n      <th>example_1</th>\n      <th>example_2</th>\n      <th>example_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "L'encodage binaire est clairement très similaire au hachage de caractéristiques, mais beaucoup plus restreint. En pratique, l'utilisation du hachage de caractéristiques est souvent conseillée par rapport à l'encodage binaire en raison du contrôle que vous avez sur les dimensions de sortie."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "binary_enc = BinaryEncoder(cols=columns).fit(X_train, y_train)",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_binary = binary_enc.transform(X_train.reset_index(drop=True))\nX_val_binary = binary_enc.transform(X_val.reset_index(drop=True))\n# note: category_encoders implementations can't handle shuffled datasets. ",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('Features after Binary Encoding: \\n', X_train_binary.shape[1])",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Features after Binary Encoding: \n 58\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nbe_logit_score = get_score(logit, X_train_binary, y_train, X_val_binary, y_val)\nprint('Logistic Regression score with binary encoding:', be_logit_score)",
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with binary encoding: 0.6410124457048876\nCPU times: user 308 ms, sys: 12 ms, total: 320 ms\nWall time: 302 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nbinary_rf_score = get_score(rf, X_train_binary, y_train, X_val_binary, y_val)\nprint('Random Forest score with binary encoding:', binary_rf_score)",
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with binary encoding: 0.7455560153361729\nCPU times: user 432 ms, sys: 248 ms, total: 680 ms\nWall time: 390 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "en"
      },
      "cell_type": "markdown",
      "source": "## Target Encoding:"
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Le Target Encoding est le premier de nos encodeurs bayésiens. Il s'agit d'une famille d'encodeurs qui prennent en compte les informations sur la variable cible. Le Target Encoding peut se référer à un codeur qui considère la corrélation statistique entre les catégories individuelles d'une caractéristique catégorielle. Dans ce tutoriel, nous examinerons uniquement les encodeurs cibles qui se concentrent sur la relation entre chaque catégorie et la moyenne de la cible, car il s'agit de la variation la plus couramment utilisée du Target Encoding."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from category_encoders import TargetEncoder\n\ntarg_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_te = targ_enc.transform(X_train.reset_index(drop=True))\nX_val_te = targ_enc.transform(X_val.reset_index(drop=True))",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_te.head()",
      "execution_count": 34,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RESOURCE</th>\n      <th>MGR_ID</th>\n      <th>ROLE_FAMILY_DESC</th>\n      <th>ROLE_FAMILY</th>\n      <th>ROLE_CODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.941675</td>\n      <td>0.972658</td>\n      <td>0.996885</td>\n      <td>0.971549</td>\n      <td>0.993056</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.749391</td>\n      <td>0.828075</td>\n      <td>0.758443</td>\n      <td>0.864438</td>\n      <td>0.777108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.966019</td>\n      <td>0.991365</td>\n      <td>0.923077</td>\n      <td>0.946237</td>\n      <td>0.910714</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.897076</td>\n      <td>0.977980</td>\n      <td>0.967211</td>\n      <td>0.910612</td>\n      <td>0.931873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.818242</td>\n      <td>0.570837</td>\n      <td>0.570837</td>\n      <td>0.864438</td>\n      <td>0.851711</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   RESOURCE    MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n0  0.941675  0.972658          0.996885     0.971549   0.993056\n1  0.749391  0.828075          0.758443     0.864438   0.777108\n2  0.966019  0.991365          0.923077     0.946237   0.910714\n3  0.897076  0.977980          0.967211     0.910612   0.931873\n4  0.818242  0.570837          0.570837     0.864438   0.851711"
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nte_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\nprint('Logistic Regression score with target encoding:', te_logit_score)",
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with target encoding: 0.8369910052854271\nCPU times: user 84 ms, sys: 12 ms, total: 96 ms\nWall time: 82 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nte_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\nprint('Random Forest score with target encoding:', te_rf_score)",
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with target encoding: 0.767684128958799\nCPU times: user 292 ms, sys: 272 ms, total: 564 ms\nWall time: 267 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "En raison de l'utilisation de la variable cible, la fuite de données et le sur-ajustement sont une préoccupation majeure. L'implémentation de category_encoders a deux façons prédéfinies de régulariser les encodages, le smoothing «lissage» et les min_samples_leaf. Ces paramètres peuvent être traités comme des hyperparamètres.\n\nle «lissage» détermine la pondération de la moyenne de chaque catégorie avec la moyenne de l'ensemble de la variable catégorielle. Il s'agit d'empêcher l'influence de moyens peu fiables provenant de catégories à faible taille d'échantillon.\n\n'min_ samples_leaf' est le nombre minimum d'échantillons dans une catégorie pour tenir compte de sa moyenne."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "targ_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)\n\nX_train_te = targ_enc.transform(X_train.reset_index(drop=True))\nX_val_te = targ_enc.transform(X_val.reset_index(drop=True))",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nme_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\nprint('Logistic Regression score with target encoding with regularization:', me_logit_score)",
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with target encoding with regularization: 0.8369910052854271\nCPU times: user 80 ms, sys: 12 ms, total: 92 ms\nWall time: 78.3 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nme_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\nprint('Random Forest score with target encoding with regularization:', me_rf_score)",
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with target encoding with regularization: 0.767684128958799\nCPU times: user 328 ms, sys: 240 ms, total: 568 ms\nWall time: 266 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "Une autre approche pour régulariser l'encodeur cible consiste à calculer la relation statistique entre chaque catégorie et la variable cible via une division kfold. Cette méthode n'est actuellement pas disponible dans l'implémentation category_encoders et doit être écrite à partir de zéro."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold\n\n# Create 5 kfold splits:\nkf = KFold(random_state=17, n_splits=5, shuffle=False)",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create copy of data:\nX_train_te = X_train.copy()\nX_train_te['target'] = y_train",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_set = []\n\nfor train_index, val_index in kf.split(X_train_te):\n    # Create splits:\n    train, val = X_train_te.iloc[train_index], X_train_te.iloc[val_index]\n    val=val.copy()\n    \n    # Calculate the mean of each column:\n    means_list = []\n    for col in columns:\n        means_list.append(train.groupby(str(col)).target.mean())\n    \n    # Calculate the mean of each category in each column:\n    col_means = []\n    for means_series in means_list:\n        col_means.append(means_series.mean())\n    \n    # Encode the data:\n    for column, means_series, means in zip(columns, means_list, col_means):\n        val[str(column) + '_target_enc'] = val[str(column)].map(means_series).fillna(means) \n    \n    list_of_mean_enc = [str(column) + '_target_enc' for column in columns]\n    list_of_mean_enc.extend(columns)\n    \n    all_set.append(val[list_of_mean_enc].copy())\n\nX_train_te=pd.concat(all_set, axis=0)",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Apply encodings to validation set:\nX_val_te = pd.DataFrame(index=X_val.index)\nfor column, means in zip(columns, col_means):\n    enc_dict = X_train_te.groupby(column).mean().to_dict()[str(column) + '_target_enc']\n    X_val_te[column] = X_val[column].map(enc_dict).fillna(means)",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create list of target encoded columns:\nlist_of_target_enc = [str(column) + '_target_enc' for column in columns]",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nkf_reg_logit_score = get_score(logit, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\nprint('Logistic Regression score with kfold-regularized target encoding:', kf_reg_logit_score)",
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with kfold-regularized target encoding: 0.8433289493105047\nCPU times: user 88 ms, sys: 8 ms, total: 96 ms\nWall time: 84.7 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nkf_reg_rf_score = get_score(rf, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\nprint('Random Forest score with kfold-regularized target encoding:', kf_reg_rf_score)",
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with kfold-regularized target encoding: 0.7747987215170301\nCPU times: user 412 ms, sys: 240 ms, total: 652 ms\nWall time: 356 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "en"
      },
      "cell_type": "markdown",
      "source": "## Weight Of Evidence (WOE):"
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "L'encodeur WOE (Weight of evidence) calcule le logarithme naturel du% de non-événements divisé par le% d'événements pour chaque catégorie dans une caractéristique catégorielle. Pour plus de précision, les événements se réfèrent à la variable cible."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from category_encoders import WOEEncoder\n\nwoe_enc = WOEEncoder(cols=columns, random_state=17).fit(X_train, y_train)",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_woe = woe_enc.transform(X_train.reset_index(drop=True))\nX_val_woe = woe_enc.transform(X_val.reset_index(drop=True))",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_woe.head()",
      "execution_count": 49,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RESOURCE</th>\n      <th>MGR_ID</th>\n      <th>ROLE_FAMILY_DESC</th>\n      <th>ROLE_FAMILY</th>\n      <th>ROLE_CODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>-0.834486</td>\n      <td>0.586900</td>\n      <td>0.734514</td>\n      <td>1.496270</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.799567</td>\n      <td>-1.933098</td>\n      <td>-1.835935</td>\n      <td>-0.933496</td>\n      <td>-1.550448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.500515</td>\n      <td>0.215336</td>\n      <td>-0.317557</td>\n      <td>0.070517</td>\n      <td>-0.480253</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.732703</td>\n      <td>-0.477811</td>\n      <td>-1.394102</td>\n      <td>-0.464076</td>\n      <td>-0.180961</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.338012</td>\n      <td>-3.696687</td>\n      <td>-3.696687</td>\n      <td>-0.933496</td>\n      <td>-1.053175</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   RESOURCE    MGR_ID  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE\n0  0.000000 -0.834486          0.586900     0.734514   1.496270\n1 -1.799567 -1.933098         -1.835935    -0.933496  -1.550448\n2  0.500515  0.215336         -0.317557     0.070517  -0.480253\n3 -0.732703 -0.477811         -1.394102    -0.464076  -0.180961\n4 -1.338012 -3.696687         -3.696687    -0.933496  -1.053175"
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nwoe_logit_score = get_score(logit, X_train_woe, y_train, X_val_woe, y_val)\nprint('Logistic Regression score with woe encoding:', woe_logit_score)",
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Logistic Regression score with woe encoding: 0.7973584285694204\nCPU times: user 76 ms, sys: 16 ms, total: 92 ms\nWall time: 76 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\nwoe_rf_score = get_score(rf, X_train_woe, y_train, X_val_woe, y_val)\nprint('Random Forest score with woe encoding:', woe_rf_score)",
      "execution_count": 51,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Random Forest score with woe encoding: 0.7700949022336552\nCPU times: user 404 ms, sys: 244 ms, total: 648 ms\nWall time: 348 ms\n"
        }
      ]
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "En résumé, les caractéristiques catégorielles peuvent être représentées de plus de façons que le traditionnel one-hot encoding. Ces représentations ont des effets différents sur nos modèles et le choix de la représentation est spécifique à la tâche (mission ou objectif). Le hachage des caractéristiques et l'encodage binaire nous offrent des moyens d'encoder les données avec des dimensions plus faibles, ce qui est moins coûteux en termes de calcul et mieux adapté aux modèles arborescents. Le Target Encoding et le Weight Of Evidence semblent être beaucoup plus spécifiques à la tâche.\n\nVos commentaires seraient appréciés, ainsi que vos votes ! Merci."
    },
    {
      "metadata": {
        "lang": "fr"
      },
      "cell_type": "markdown",
      "source": "### Pour en savoir plus:\n\n* [category_encoder documentation](http://contrib.scikit-learn.org/categorical-encoding/)\n* [weight of evidence](https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html)\n* [smarter ways of encoding categorical data for machine learning](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)\n* [an exploration of categorical variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)"
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "base_numbering": 1,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "nbTranslate": {
      "hotkey": "alt-t",
      "sourceLang": "en",
      "targetLang": "fr",
      "displayLangs": [
        "*"
      ],
      "langInMainMenu": true,
      "useGoogleTranslate": true
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}